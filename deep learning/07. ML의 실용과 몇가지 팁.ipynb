{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 rate\n",
    "- large learning rate : 굉장히 띄엄띄엄 러닝하기 때문에 밖으로 팅겨 나가게 되는 현상이 발생 할 수 있다.\n",
    "- short learning rate : 굉장히 촘촘하게 러닝하여 오래 걸리고 다 러닝할 수 없게 되는 현상이 발생 한다.\n",
    "\n",
    "=> learning rate는 잘 조절해야함\n",
    "\n",
    "- standardization(normalization data) = (값 - 평균) / 분산\n",
    "      = x_std[:,0] = (x[:,0] - x[:,0].mean()) / x[:,0].std()\n",
    "- overfitting : 데이터에 너무 치중하여 오버된 값까지 모두 생각해서 계산하는 것 따라서  정확도가 떨어지게 되는 현상이 발생한다. 이를 해결하기 위해 학습 데이터를 늘리고 중복되는 것은 줄이고 일반화시키는 방법이 있다.\n",
    "- regularization(일반화) :구불구불한 그래프의 경우에 이것을 펴주는 것\n",
    "      = cost식에서 term을 추가시키면 된다. 각 요소의 제곱의 합과 regularization strength라는 상수를 곱해준 값이 term이다.\n",
    "          ex) l2reg = 0.001 * tf.reduce_sum(tf.square(w)\n",
    "          (0.001 : regularization strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.2 training/testing 데이터 셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":만약 여러개의 데이터가 있을 때 모든 데이터가 트레이닝에 사용되면 답은 입력된 대로 나오기 때문에 답은 고정되어있다.\n",
    "\n",
    "=> 이를 해결하려면 70퍼정도를 training 30퍼를 testing으로 두고 데이터를 이용해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training set : training set에는 앞에서 배웠던 알파나 람다 값도 다루게 되는데 이것을 처리하기 위해서 training과 validation이라는 단계를 따로 두어 training에서 학습을 시키고 validation에서 알파나 람다 값을 튜닝을 해준다. 이후에 testing에서 시험해 봄\n",
    "\n",
    "- Accuracy : testing한 값과 원래의 결과 값을 비교해서 정확도를 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 training/test dataset, learning rate, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.70868 [[-1.60347748  1.05094433 -0.27734786]\n",
      " [ 0.41523954  1.25533819 -1.21930194]\n",
      " [ 1.07775688 -0.14921755  0.94112849]]\n",
      "1 3.0253 [[-1.60979068  1.02008939 -0.24017973]\n",
      " [ 0.44603747  1.12524164 -1.12000334]\n",
      " [ 1.07589436 -0.23332471  1.02709818]]\n",
      "2 2.69324 [[-1.6355679   1.00908935 -0.20340258]\n",
      " [ 0.37021622  1.10262573 -1.02156615]\n",
      " [ 0.96847963 -0.2101047   1.11129296]]\n",
      "3 2.3827 [[-1.64499462  0.98331344 -0.16819987]\n",
      " [ 0.37763917  1.0001533  -0.92651665]\n",
      " [ 0.94842488 -0.2666555   1.18789852]]\n",
      "4 2.1123 [[-1.66489995  0.97030455 -0.13528565]\n",
      " [ 0.3219716   0.96607947 -0.83677524]\n",
      " [ 0.87098598 -0.25511321  1.25379515]]\n",
      "5 1.89965 [[-1.67262959  0.94948292 -0.10673432]\n",
      " [ 0.32230633  0.88684106 -0.75787163]\n",
      " [ 0.85763043 -0.28686818  1.29890573]]\n",
      "6 1.75281 [[-1.68585312  0.9381364  -0.08216425]\n",
      " [ 0.28530702  0.85745502 -0.69148624]\n",
      " [ 0.81354928 -0.26943713  1.3255558 ]]\n",
      "7 1.66373 [[-1.69106817  0.92349225 -0.06230504]\n",
      " [ 0.28745484  0.80682153 -0.6430006 ]\n",
      " [ 0.81203055 -0.27141756  1.32905495]]\n",
      "8 1.61438 [[-1.69866514  0.91443729 -0.04565315]\n",
      " [ 0.27399796  0.78596008 -0.60868227]\n",
      " [ 0.79772711 -0.24490239  1.31684327]]\n",
      "9 1.58165 [[-1.7029469   0.9040792  -0.03101332]\n",
      " [ 0.27839726  0.75689155 -0.58401304]\n",
      " [ 0.80162036 -0.22614357  1.29419112]]\n",
      "10 1.5525 [[-1.70863199  0.89550757 -0.01675659]\n",
      " [ 0.27485827  0.73774517 -0.56132764]\n",
      " [ 0.79819506 -0.19852929  1.27000213]]\n",
      "11 1.52424 [[-1.71334684  0.88622242 -0.00275661]\n",
      " [ 0.27691978  0.71452045 -0.54016441]\n",
      " [ 0.80024344 -0.17520912  1.24463356]]\n",
      "12 1.49657 [[-1.71866071  0.87750179  0.01127786]\n",
      " [ 0.27581137  0.69452477 -0.51906031]\n",
      " [ 0.79928124 -0.14943656  1.21982324]]\n",
      "13 1.46945 [[-1.72365832  0.86851984  0.02525749]\n",
      " [ 0.2766816   0.67308652 -0.4984923 ]\n",
      " [ 0.8002423  -0.12557964  1.1950053 ]]\n",
      "14 1.44289 [[-1.72890556  0.85977608  0.03924857]\n",
      " [ 0.27635083  0.65303779 -0.47811276]\n",
      " [ 0.80006903 -0.10102765  1.17062652]]\n",
      "15 1.4169 [[-1.73405099  0.85095882  0.05321132]\n",
      " [ 0.27678829  0.63261843 -0.4581309 ]\n",
      " [ 0.80065143 -0.0774393   1.14645576]]\n",
      "16 1.39149 [[-1.73931396  0.84226757  0.06716553]\n",
      " [ 0.27675688  0.6129564  -0.43843743]\n",
      " [ 0.80080116 -0.05378661  1.12265337]]\n",
      "17 1.36668 [[-1.74455249  0.83357674  0.08109485]\n",
      " [ 0.27704686  0.59334612 -0.41911712]\n",
      " [ 0.80127692 -0.03074844  1.09913945]]\n",
      "18 1.34248 [[-1.74985564  0.82497042  0.09500429]\n",
      " [ 0.27715003  0.57426268 -0.40013683]\n",
      " [ 0.8015914  -0.00790376  1.07598031]]\n",
      "19 1.31889 [[-1.75516248  0.81639576  0.10888582]\n",
      " [ 0.27739951  0.55541158 -0.3815352 ]\n",
      " [ 0.80206525  0.01445121  1.05315149]]\n",
      "20 1.29594 [[-1.76051044  0.80789042  0.12273911]\n",
      " [ 0.27757612  0.53700566 -0.3633059 ]\n",
      " [ 0.80248946  0.03649426  1.03068423]]\n",
      "21 1.27362 [[-1.76587141  0.7994315   0.13655904]\n",
      " [ 0.2778264   0.51891911 -0.34546959]\n",
      " [ 0.80300641  0.05808732  1.00857425]]\n",
      "22 1.25195 [[-1.7712611   0.79103696  0.15034322]\n",
      " [ 0.27805144  0.5012536  -0.32802916]\n",
      " [ 0.80352324  0.07930613  0.98683864]]\n",
      "23 1.23094 [[-1.77666533  0.78269696  0.16408752]\n",
      " [ 0.27831787  0.48395544 -0.3109974 ]\n",
      " [ 0.80410624  0.10008193  0.96547985]]\n",
      "24 1.21057 [[-1.78209031  0.77442068  0.17778875]\n",
      " [ 0.27857888  0.46707654 -0.29437947]\n",
      " [ 0.80471313  0.12044647  0.94450843]]\n",
      "25 1.19086 [[-1.78752816  0.76620412  0.19144313]\n",
      " [ 0.27886531  0.45059446 -0.27818382]\n",
      " [ 0.80537635  0.14036316  0.92392856]]\n",
      "26 1.17181 [[-1.79298055  0.75805247  0.20504716]\n",
      " [ 0.27915382  0.43453741 -0.26241529]\n",
      " [ 0.80607635  0.15984483  0.9037469 ]]\n",
      "27 1.15341 [[-1.7984426   0.74996442  0.21859722]\n",
      " [ 0.27945852  0.41889644 -0.24707904]\n",
      " [ 0.80682963  0.1788712   0.88396728]]\n",
      "28 1.13566 [[-1.80391395  0.7419433   0.23208974]\n",
      " [ 0.27976689  0.4036873  -0.23217827]\n",
      " [ 0.80762714  0.19744737  0.86459357]]\n",
      "29 1.11855 [[-1.80939114  0.73398894  0.24552125]\n",
      " [ 0.28008515  0.38890621 -0.21771544]\n",
      " [ 0.80847776  0.21556216  0.84562814]]\n",
      "30 1.10207 [[-1.81487298  0.72610378  0.25888827]\n",
      " [ 0.28040576  0.3745617  -0.20369151]\n",
      " [ 0.80937701  0.23321828  0.8270728 ]]\n",
      "31 1.08621 [[-1.82035673  0.71828824  0.2721875 ]\n",
      " [ 0.28073114  0.36065125 -0.19010642]\n",
      " [ 0.81032979  0.25041029  0.80892801]]\n",
      "32 1.07096 [[-1.82584083  0.71054417  0.28541565]\n",
      " [ 0.28105631  0.3471784  -0.17695874]\n",
      " [ 0.81133372  0.26714084  0.7911936 ]]\n",
      "33 1.05631 [[-1.83132291  0.70287228  0.29856965]\n",
      " [ 0.28138173  0.3341403  -0.16424605]\n",
      " [ 0.81239122  0.28340873  0.7738682 ]]\n",
      "34 1.04224 [[-1.83680153  0.695274    0.31164652]\n",
      " [ 0.28170395  0.32153669 -0.15196465]\n",
      " [ 0.81350088  0.29921749  0.75694978]]\n",
      "35 1.02873 [[-1.84227467  0.68775016  0.32464346]\n",
      " [ 0.28202239  0.30936351 -0.14010991]\n",
      " [ 0.81466365  0.31456921  0.7404353 ]]\n",
      "36 1.01577 [[-1.84774077  0.68030196  0.33755782]\n",
      " [ 0.28233448  0.2976177  -0.12867618]\n",
      " [ 0.81587821  0.32946885  0.72432107]]\n",
      "37 1.00334 [[-1.85319829  0.67293012  0.35038719]\n",
      " [ 0.28263938  0.28629348 -0.11765685]\n",
      " [ 0.81714463  0.3439208   0.70860273]]\n",
      "38 0.991414 [[-1.8586458   0.66563559  0.36312926]\n",
      " [ 0.28293502  0.27538568 -0.10704468]\n",
      " [ 0.81846142  0.3579317   0.69327503]]\n",
      "39 0.979983 [[-1.86408198  0.65841901  0.37578198]\n",
      " [ 0.28322068  0.26488695 -0.09683158]\n",
      " [ 0.81982803  0.37150773  0.67833239]]\n",
      "40 0.969023 [[-1.86950564  0.65128112  0.38834348]\n",
      " [ 0.28349468  0.25479028 -0.08700892]\n",
      " [ 0.82124281  0.38465685  0.66376847]]\n",
      "41 0.958516 [[-1.8749156   0.64422244  0.40081209]\n",
      " [ 0.28375632  0.24508724 -0.0775675 ]\n",
      " [ 0.82270467  0.39738682  0.6495766 ]]\n",
      "42 0.948441 [[-1.88031089  0.63724345  0.41318634]\n",
      " [ 0.28400448  0.23576927 -0.0684977 ]\n",
      " [ 0.8242119   0.40970644  0.63574976]]\n",
      "43 0.938779 [[-1.88569057  0.63034451  0.42546493]\n",
      " [ 0.28423858  0.2268271  -0.05978962]\n",
      " [ 0.82576299  0.42162466  0.62228042]]\n",
      "44 0.929513 [[-1.8910538   0.62352592  0.43764678]\n",
      " [ 0.28445783  0.21825126 -0.05143302]\n",
      " [ 0.82735604  0.43315107  0.60916096]]\n",
      "45 0.920622 [[-1.89639986  0.61678785  0.44973096]\n",
      " [ 0.28466189  0.21003176 -0.04341756]\n",
      " [ 0.82898939  0.44429529  0.59638339]]\n",
      "46 0.912089 [[-1.90172815  0.61013037  0.46171674]\n",
      " [ 0.28485039  0.20215842 -0.03573273]\n",
      " [ 0.83066118  0.45506725  0.58393967]]\n",
      "47 0.903897 [[-1.90703821  0.60355353  0.47360358]\n",
      " [ 0.28502303  0.19462106 -0.02836802]\n",
      " [ 0.83236939  0.46547714  0.57182151]]\n",
      "48 0.896029 [[-1.91232944  0.59705728  0.48539105]\n",
      " [ 0.28517982  0.18740916 -0.02131291]\n",
      " [ 0.83411223  0.47553509  0.56002069]]\n",
      "49 0.888469 [[-1.91760147  0.59064138  0.49707893]\n",
      " [ 0.2853207   0.18051231 -0.01455693]\n",
      " [ 0.83588773  0.48525137  0.54852891]]\n",
      "50 0.8812 [[-1.92285395  0.58430564  0.50866705]\n",
      " [ 0.28544581  0.17392011 -0.00808981]\n",
      " [ 0.83769405  0.49463624  0.53733772]]\n",
      "51 0.874209 [[ -1.92808652e+00   5.78049779e-01   5.20155489e-01]\n",
      " [  2.85555243e-01   1.67622119e-01  -1.90126756e-03]\n",
      " [  8.39529157e-01   5.03699839e-01   5.26438951e-01]]\n",
      "52 0.86748 [[-1.93329906  0.57187343  0.53154439]\n",
      " [ 0.28564933  0.1616081   0.00401869]\n",
      " [ 0.84139132  0.5124523   0.51582438]]\n",
      "53 0.861 [[-1.93849146  0.56577611  0.54283404]\n",
      " [ 0.28572834  0.15586793  0.00967985]\n",
      " [ 0.84327859  0.52090365  0.50548577]]\n",
      "54 0.854757 [[-1.94366348  0.55975735  0.55402482]\n",
      " [ 0.28579271  0.15039158  0.01509182]\n",
      " [ 0.84518927  0.52906358  0.49541515]]\n",
      "55 0.848738 [[-1.94881511  0.55381662  0.56511718]\n",
      " [ 0.28584281  0.14516935  0.02026396]\n",
      " [ 0.84712148  0.53694195  0.48560458]]\n",
      "56 0.842932 [[-1.95394623  0.54795325  0.57611167]\n",
      " [ 0.28587925  0.14019154  0.02520532]\n",
      " [ 0.84907371  0.54454803  0.47604626]]\n",
      "57 0.837327 [[-1.95905697  0.54216665  0.58700895]\n",
      " [ 0.28590238  0.13544896  0.02992478]\n",
      " [ 0.85104412  0.55189133  0.46673262]]\n",
      "58 0.831913 [[-1.96414721  0.53645617  0.59780973]\n",
      " [ 0.2859129   0.13093238  0.03443085]\n",
      " [ 0.85303128  0.55898064  0.45765615]]\n",
      "59 0.826681 [[-1.96921706  0.53082103  0.60851479]\n",
      " [ 0.2859112   0.12663306  0.03873186]\n",
      " [ 0.85503352  0.56582505  0.44880956]]\n",
      "60 0.821621 [[-1.97426665  0.52526051  0.61912495]\n",
      " [ 0.28589803  0.12254228  0.04283579]\n",
      " [ 0.85704947  0.57243294  0.44018573]]\n",
      "61 0.816724 [[-1.97929609  0.51977378  0.62964112]\n",
      " [ 0.28587392  0.11865182  0.04675037]\n",
      " [ 0.85907763  0.57881278  0.43177772]]\n",
      "62 0.811982 [[-1.9843055   0.51436013  0.64006418]\n",
      " [ 0.28583944  0.11495361  0.05048304]\n",
      " [ 0.86111671  0.58497268  0.42357877]]\n",
      "63 0.807388 [[-1.98929501  0.50901866  0.65039515]\n",
      " [ 0.28579533  0.11143975  0.05404101]\n",
      " [ 0.8631655   0.59092039  0.4155823 ]]\n",
      "64 0.802933 [[-1.99426472  0.50374854  0.66063499]\n",
      " [ 0.28574198  0.1081029   0.05743121]\n",
      " [ 0.86522251  0.59666371  0.40778196]]\n",
      "65 0.798613 [[-1.99921489  0.49854892  0.67078477]\n",
      " [ 0.28568029  0.10493555  0.06066026]\n",
      " [ 0.86728692  0.60220981  0.40017149]]\n",
      "66 0.794418 [[-2.00414562  0.49341899  0.6808455 ]\n",
      " [ 0.2856105   0.10193103  0.06373458]\n",
      " [ 0.86935723  0.60756612  0.3927449 ]]\n",
      "67 0.790345 [[-2.00905728  0.48835778  0.69081831]\n",
      " [ 0.28553361  0.09908217  0.06666034]\n",
      " [ 0.87143272  0.61273915  0.38549635]]\n",
      "68 0.786386 [[-2.01394987  0.48336446  0.70070428]\n",
      " [ 0.2854498   0.09638286  0.06944343]\n",
      " [ 0.87351215  0.61773592  0.37842017]]\n",
      "69 0.782537 [[-2.01882362  0.47843814  0.71050447]\n",
      " [ 0.28535995  0.0938266   0.07208956]\n",
      " [ 0.87559474  0.62256265  0.37151089]]\n",
      "70 0.778791 [[-2.02367902  0.47357792  0.72022003]\n",
      " [ 0.2852644   0.09140753  0.07460418]\n",
      " [ 0.87767941  0.6272257   0.36476317]]\n",
      "71 0.775146 [[-2.02851605  0.46878287  0.72985208]\n",
      " [ 0.28516382  0.08911975  0.07699256]\n",
      " [ 0.87976545  0.63173091  0.35817191]]\n",
      "72 0.771595 [[-2.03333497  0.46405214  0.7394017 ]\n",
      " [ 0.28505874  0.08695771  0.07925969]\n",
      " [ 0.88185203  0.63608414  0.35173211]]\n",
      "73 0.768136 [[-2.03813601  0.45938486  0.74887002]\n",
      " [ 0.28494954  0.0849162   0.08141038]\n",
      " [ 0.88393831  0.64029098  0.34543896]]\n",
      "74 0.764763 [[-2.0429194   0.4547801   0.75825816]\n",
      " [ 0.28483686  0.08298998  0.0834493 ]\n",
      " [ 0.8860237   0.64435673  0.33928782]]\n",
      "75 0.761472 [[-2.04768538  0.45023698  0.76756728]\n",
      " [ 0.28472108  0.08117415  0.08538091]\n",
      " [ 0.88810748  0.64828664  0.33327416]]\n",
      "76 0.758262 [[-2.05243421  0.44575465  0.77679843]\n",
      " [ 0.28460267  0.07946402  0.08720946]\n",
      " [ 0.89018899  0.6520856   0.32739365]]\n",
      "77 0.755127 [[-2.0571661   0.44133222  0.78595275]\n",
      " [ 0.28448209  0.07785503  0.08893904]\n",
      " [ 0.8922677   0.6557585   0.3216421 ]]\n",
      "78 0.752065 [[-2.0618813   0.43696883  0.79503131]\n",
      " [ 0.28435972  0.07634287  0.09057357]\n",
      " [ 0.89434302  0.65930992  0.31601539]]\n",
      "79 0.749072 [[-2.06658006  0.43266362  0.80403519]\n",
      " [ 0.28423604  0.07492325  0.09211687]\n",
      " [ 0.89641446  0.66274416  0.31050968]]\n",
      "80 0.746147 [[-2.0712626   0.42841578  0.81296545]\n",
      " [ 0.28411126  0.07359241  0.09357249]\n",
      " [ 0.89848143  0.66606581  0.30512106]]\n",
      "81 0.743286 [[-2.07592893  0.42422441  0.82182324]\n",
      " [ 0.28398597  0.07234622  0.09494396]\n",
      " [ 0.90054369  0.66927862  0.29984596]]\n",
      "82 0.740486 [[-2.08057952  0.42008874  0.83060956]\n",
      " [ 0.28386033  0.07118127  0.09623457]\n",
      " [ 0.90260065  0.67238688  0.29468077]]\n",
      "83 0.737746 [[-2.08521461  0.41600791  0.83932543]\n",
      " [ 0.28373474  0.07009388  0.09744754]\n",
      " [ 0.904652    0.67539424  0.28962207]]\n",
      "84 0.735063 [[-2.08983421  0.41198114  0.84797192]\n",
      " [ 0.28360951  0.06908069  0.09858595]\n",
      " [ 0.90669739  0.67830431  0.28466657]]\n",
      "85 0.732435 [[-2.09443879  0.40800762  0.85654998]\n",
      " [ 0.28348491  0.0681385   0.09965274]\n",
      " [ 0.90873641  0.68112075  0.27981105]]\n",
      "86 0.729859 [[-2.09902835  0.40408656  0.86506069]\n",
      " [ 0.28336126  0.06726416  0.10065072]\n",
      " [ 0.91076887  0.68384689  0.27505243]]\n",
      "87 0.727334 [[-2.10360336  0.40021721  0.873505  ]\n",
      " [ 0.28323874  0.06645472  0.10158265]\n",
      " [ 0.91279435  0.68648607  0.27038774]]\n",
      "88 0.724858 [[-2.10816383  0.39639875  0.88188386]\n",
      " [ 0.28311774  0.06570725  0.10245111]\n",
      " [ 0.9148128   0.68904132  0.26581407]]\n",
      "89 0.72243 [[-2.11271     0.39263052  0.89019823]\n",
      " [ 0.28299835  0.06501916  0.10325862]\n",
      " [ 0.9168238   0.6915158   0.26132864]]\n",
      "90 0.720047 [[-2.1172421   0.38891169  0.89844906]\n",
      " [ 0.28288084  0.0643877   0.10400759]\n",
      " [ 0.91882718  0.69391227  0.25692877]]\n",
      "91 0.717708 [[-2.12176013  0.3852416   0.90663731]\n",
      " [ 0.28276542  0.06381042  0.1047003 ]\n",
      " [ 0.92082274  0.69623363  0.25261182]]\n",
      "92 0.715411 [[-2.12626457  0.38161948  0.91476387]\n",
      " [ 0.28265229  0.06328483  0.10533904]\n",
      " [ 0.92281038  0.69848251  0.2483753 ]]\n",
      "93 0.713156 [[-2.13075542  0.37804466  0.92282957]\n",
      " [ 0.28254154  0.06280874  0.1059259 ]\n",
      " [ 0.92478985  0.7006616   0.24421676]]\n",
      "94 0.71094 [[-2.13523293  0.37451643  0.93083537]\n",
      " [ 0.28243339  0.06237983  0.10646294]\n",
      " [ 0.92676103  0.70277333  0.24013385]]\n",
      "95 0.708763 [[-2.13969731  0.37103412  0.9387821 ]\n",
      " [ 0.28232798  0.06199606  0.10695212]\n",
      " [ 0.92872381  0.7048201   0.23612429]]\n",
      "96 0.706623 [[-2.14414883  0.36759701  0.94667059]\n",
      " [ 0.28222552  0.06165528  0.10739538]\n",
      " [ 0.93067813  0.70680416  0.23218592]]\n",
      "97 0.704518 [[-2.14858747  0.36420453  0.95450175]\n",
      " [ 0.28212595  0.06135574  0.1077945 ]\n",
      " [ 0.93262374  0.70872796  0.22831656]]\n",
      "98 0.702449 [[-2.15301347  0.36085591  0.96227634]\n",
      " [ 0.28202969  0.06109523  0.10815129]\n",
      " [ 0.93456084  0.71059322  0.22451422]]\n",
      "99 0.700413 [[-2.15742707  0.35755062  0.96999514]\n",
      " [ 0.28193644  0.06087242  0.10846736]\n",
      " [ 0.93648893  0.71240252  0.22077683]]\n",
      "100 0.698411 [[-2.16182828  0.35428795  0.97765899]\n",
      " [ 0.28184673  0.06068508  0.10874441]\n",
      " [ 0.93840843  0.71415728  0.21710256]]\n",
      "101 0.69644 [[-2.16621733  0.35106736  0.98526865]\n",
      " [ 0.28176016  0.06053212  0.10898393]\n",
      " [ 0.94031882  0.71585995  0.21348947]]\n",
      "102 0.6945 [[-2.17059445  0.34788817  0.99282491]\n",
      " [ 0.28167731  0.06041141  0.10918748]\n",
      " [ 0.94222051  0.71751189  0.20993583]]\n",
      "103 0.692589 [[-2.17495966  0.34474984  1.00032854]\n",
      " [ 0.28159788  0.06032185  0.10935647]\n",
      " [ 0.94411319  0.7191152   0.20643985]]\n",
      "104 0.690708 [[-2.17931318  0.34165174  1.00778019]\n",
      " [ 0.28152207  0.0602618   0.10949232]\n",
      " [ 0.94599694  0.72067142  0.20299989]]\n",
      "105 0.688855 [[-2.18365526  0.33859333  1.01518059]\n",
      " [ 0.28144988  0.06022995  0.10959634]\n",
      " [ 0.94787169  0.72218221  0.1996143 ]]\n",
      "106 0.68703 [[-2.1879859   0.33557403  1.02253056]\n",
      " [ 0.28138149  0.06022488  0.10966983]\n",
      " [ 0.94973755  0.72364914  0.19628152]]\n",
      "107 0.685231 [[-2.19230533  0.33259326  1.02983069]\n",
      " [ 0.28131676  0.06024542  0.10971405]\n",
      " [ 0.95159441  0.72507381  0.19300002]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 0.683458 [[-2.19661355  0.32965049  1.03708172]\n",
      " [ 0.28125578  0.06029026  0.10973018]\n",
      " [ 0.95344234  0.72645754  0.18976834]]\n",
      "109 0.68171 [[-2.20091081  0.32674518  1.04428422]\n",
      " [ 0.28119865  0.0603582   0.10971936]\n",
      " [ 0.95528144  0.72780174  0.18658504]]\n",
      "110 0.679988 [[-2.2051971   0.32387683  1.05143893]\n",
      " [ 0.28114522  0.06044831  0.10968269]\n",
      " [ 0.95711154  0.72910792  0.18344873]]\n",
      "111 0.678289 [[-2.20947266  0.32104483  1.05854654]\n",
      " [ 0.28109571  0.06055924  0.10962124]\n",
      " [ 0.95893294  0.73037714  0.1803581 ]]\n",
      "112 0.676613 [[-2.21373773  0.31824878  1.06560767]\n",
      " [ 0.28104985  0.06069031  0.10953606]\n",
      " [ 0.96074533  0.73161101  0.17731185]]\n",
      "113 0.67496 [[-2.21799231  0.31548807  1.0726229 ]\n",
      " [ 0.28100803  0.06084006  0.10942812]\n",
      " [ 0.96254921  0.73281026  0.17430875]]\n",
      "114 0.673329 [[-2.22223639  0.31276232  1.07959282]\n",
      " [ 0.28096977  0.0610081   0.10929836]\n",
      " [ 0.96434414  0.73397654  0.17134753]]\n",
      "115 0.67172 [[-2.22647023  0.31007096  1.08651805]\n",
      " [ 0.28093547  0.06119301  0.10914773]\n",
      " [ 0.96613061  0.73511052  0.16842709]]\n",
      "116 0.670132 [[-2.23069406  0.30741355  1.09339917]\n",
      " [ 0.28090483  0.06139428  0.1089771 ]\n",
      " [ 0.96790832  0.73621362  0.16554625]]\n",
      "117 0.668565 [[-2.23490763  0.3047896   1.10023677]\n",
      " [ 0.28087807  0.06161084  0.1087873 ]\n",
      " [ 0.96967763  0.73728669  0.16270392]]\n",
      "118 0.667018 [[-2.23911142  0.30219868  1.10703146]\n",
      " [ 0.28085494  0.06184212  0.10857917]\n",
      " [ 0.97143835  0.73833084  0.15989903]]\n",
      "119 0.66549 [[-2.24330521  0.2996403   1.11378372]\n",
      " [ 0.2808356   0.06208712  0.1083535 ]\n",
      " [ 0.97319072  0.73934692  0.15713055]]\n",
      "120 0.663981 [[-2.24748945  0.29711404  1.12049413]\n",
      " [ 0.28081989  0.06234527  0.10811106]\n",
      " [ 0.97493476  0.74033594  0.15439752]]\n",
      "121 0.662491 [[-2.25166392  0.29461944  1.12716329]\n",
      " [ 0.28080785  0.0626158   0.10785257]\n",
      " [ 0.97667056  0.74129874  0.15169893]]\n",
      "122 0.66102 [[-2.25582886  0.2921561   1.13379169]\n",
      " [ 0.28079942  0.06289808  0.10757872]\n",
      " [ 0.97839814  0.7422362   0.14903384]]\n",
      "123 0.659566 [[-2.25998449  0.28972358  1.14037979]\n",
      " [ 0.28079456  0.06319146  0.10729021]\n",
      " [ 0.98011762  0.74314922  0.14640138]]\n",
      "124 0.65813 [[-2.26413083  0.28732145  1.14692819]\n",
      " [ 0.28079325  0.06349526  0.10698771]\n",
      " [ 0.98182911  0.74403846  0.14380065]]\n",
      "125 0.65671 [[-2.26826787  0.2849493   1.15343738]\n",
      " [ 0.28079545  0.06380895  0.10667183]\n",
      " [ 0.98353267  0.74490476  0.14123078]]\n",
      "126 0.655308 [[-2.27239561  0.28260675  1.1599077 ]\n",
      " [ 0.28080106  0.06413201  0.10634318]\n",
      " [ 0.98522836  0.74574894  0.13869096]]\n",
      "127 0.653921 [[-2.27651429  0.28029341  1.16633976]\n",
      " [ 0.28081009  0.06446381  0.10600235]\n",
      " [ 0.9869163   0.74657154  0.1361804 ]]\n",
      "128 0.652551 [[-2.28062415  0.27800885  1.17273414]\n",
      " [ 0.28082246  0.06480386  0.10564994]\n",
      " [ 0.98859656  0.7473734   0.13369833]]\n",
      "129 0.651196 [[-2.28472495  0.27575272  1.1790911 ]\n",
      " [ 0.28083813  0.06515169  0.10528645]\n",
      " [ 0.99026924  0.74815512  0.13124397]]\n",
      "130 0.649857 [[-2.28881693  0.27352464  1.18541121]\n",
      " [ 0.28085703  0.06550683  0.10491242]\n",
      " [ 0.99193442  0.74891734  0.12881662]]\n",
      "131 0.648532 [[-2.29290032  0.27132422  1.19169497]\n",
      " [ 0.28087914  0.06586879  0.10452835]\n",
      " [ 0.99359214  0.74966067  0.12641555]]\n",
      "132 0.647222 [[-2.2969749   0.26915109  1.19794273]\n",
      " [ 0.28090444  0.06623709  0.10413473]\n",
      " [ 0.99524266  0.75038564  0.12404008]]\n",
      "133 0.645927 [[-2.30104089  0.26700494  1.20415497]\n",
      " [ 0.28093269  0.06661151  0.10373206]\n",
      " [ 0.99688578  0.75109303  0.12168957]]\n",
      "134 0.644645 [[-2.30509853  0.26488534  1.21033204]\n",
      " [ 0.28096417  0.06699135  0.10332076]\n",
      " [ 0.99852198  0.75178307  0.11936335]]\n",
      "135 0.643378 [[-2.3091476   0.26279202  1.21647441]\n",
      " [ 0.28099838  0.06737661  0.10290128]\n",
      " [ 1.00015092  0.75245667  0.1170608 ]]\n",
      "136 0.642124 [[-2.31318831  0.26072457  1.22258258]\n",
      " [ 0.28103572  0.06776649  0.10247406]\n",
      " [ 1.00177312  0.75311393  0.11478134]]\n",
      "137 0.640883 [[-2.31722069  0.25868276  1.22865689]\n",
      " [ 0.28107569  0.06816112  0.10203945]\n",
      " [ 1.00338829  0.75375581  0.11252434]]\n",
      "138 0.639655 [[-2.32124496  0.25666612  1.2346977 ]\n",
      " [ 0.28111866  0.06855969  0.1015979 ]\n",
      " [ 1.0049969   0.75438231  0.11028928]]\n",
      "139 0.63844 [[-2.32526112  0.25467446  1.24070549]\n",
      " [ 0.28116417  0.06896234  0.10114975]\n",
      " [ 1.00659859  0.75499433  0.10807557]]\n",
      "140 0.637237 [[-2.32926917  0.25270736  1.24668062]\n",
      " [ 0.28121254  0.06936834  0.10069539]\n",
      " [ 1.00819385  0.75559187  0.10588271]]\n",
      "141 0.636046 [[-2.33326912  0.25076455  1.25262344]\n",
      " [ 0.28126341  0.06977772  0.10023513]\n",
      " [ 1.00978255  0.7561757   0.10371013]]\n",
      "142 0.634868 [[-2.3372612   0.24884571  1.25853431]\n",
      " [ 0.28131685  0.07019008  0.09976935]\n",
      " [ 1.01136494  0.75674611  0.10155735]]\n",
      "143 0.633701 [[-2.34124541  0.24695052  1.2644136 ]\n",
      " [ 0.28137279  0.07060513  0.09929835]\n",
      " [ 1.012941    0.75730348  0.0994239 ]]\n",
      "144 0.632546 [[-2.34522176  0.2450787   1.27026165]\n",
      " [ 0.28143114  0.07102264  0.09882247]\n",
      " [ 1.01451087  0.7578482   0.09730931]]\n",
      "145 0.631402 [[-2.34919024  0.24322996  1.27607894]\n",
      " [ 0.28149182  0.07144245  0.098342  ]\n",
      " [ 1.01607454  0.75838077  0.09521309]]\n",
      "146 0.630269 [[-2.35315108  0.24140397  1.28186572]\n",
      " [ 0.28155494  0.07186412  0.09785722]\n",
      " [ 1.01763225  0.7589013   0.09313481]]\n",
      "147 0.629148 [[-2.3571043   0.23960048  1.28762233]\n",
      " [ 0.2816202   0.07228767  0.09736842]\n",
      " [ 1.01918399  0.75941038  0.09107403]]\n",
      "148 0.628036 [[-2.36104989  0.23781919  1.29334915]\n",
      " [ 0.28168771  0.07271274  0.09687585]\n",
      " [ 1.0207299   0.75990814  0.08903034]]\n",
      "149 0.626936 [[-2.36498785  0.23605984  1.29904652]\n",
      " [ 0.28175727  0.07313922  0.0963798 ]\n",
      " [ 1.02226996  0.76039505  0.08700333]]\n",
      "150 0.625846 [[-2.36891842  0.23432215  1.3047148 ]\n",
      " [ 0.28182894  0.07356685  0.09588051]\n",
      " [ 1.02380443  0.76087135  0.0849926 ]]\n",
      "151 0.624766 [[-2.3728416   0.23260583  1.31035423]\n",
      " [ 0.28190261  0.0739955   0.09537821]\n",
      " [ 1.02533329  0.76133734  0.08299775]]\n",
      "152 0.623696 [[-2.37675738  0.23091063  1.31596518]\n",
      " [ 0.28197828  0.0744249   0.09487313]\n",
      " [ 1.02685666  0.76179326  0.08101844]]\n",
      "153 0.622635 [[-2.38066578  0.2292363   1.32154787]\n",
      " [ 0.28205574  0.0748551   0.0943655 ]\n",
      " [ 1.02837455  0.76223958  0.07905427]]\n",
      "154 0.621585 [[-2.38456678  0.22758256  1.32710278]\n",
      " [ 0.28213513  0.07528566  0.09385554]\n",
      " [ 1.0298872   0.7626763   0.07710492]]\n",
      "155 0.620543 [[-2.38846064  0.22594918  1.33263004]\n",
      " [ 0.28221619  0.07571669  0.09334347]\n",
      " [ 1.03139448  0.7631039   0.07517005]]\n",
      "156 0.619512 [[-2.39234734  0.22433588  1.33813012]\n",
      " [ 0.28229907  0.07614781  0.09282948]\n",
      " [ 1.03289664  0.76352245  0.07324932]]\n",
      "157 0.618489 [[-2.39622688  0.22274245  1.34360313]\n",
      " [ 0.28238353  0.07657914  0.09231371]\n",
      " [ 1.03439367  0.76393241  0.07134236]]\n",
      "158 0.617475 [[-2.40009952  0.22116861  1.34904957]\n",
      " [ 0.28246972  0.07701024  0.09179643]\n",
      " [ 1.03588581  0.76433367  0.06944895]]\n",
      "159 0.61647 [[-2.403965    0.21961418  1.35446954]\n",
      " [ 0.28255728  0.07744139  0.09127773]\n",
      " [ 1.03737283  0.76472694  0.06756868]]\n",
      "160 0.615474 [[-2.40782356  0.21807885  1.3598634 ]\n",
      " [ 0.28264654  0.07787201  0.09075786]\n",
      " [ 1.0388552   0.76511192  0.06570136]]\n",
      "161 0.614487 [[-2.41167521  0.21656246  1.36523139]\n",
      " [ 0.28273702  0.07830244  0.09023693]\n",
      " [ 1.04033256  0.76548928  0.06384663]]\n",
      "162 0.613507 [[-2.41551995  0.21506472  1.37057388]\n",
      " [ 0.28282911  0.07873214  0.08971515]\n",
      " [ 1.04180539  0.76585877  0.06200426]]\n",
      "163 0.612536 [[-2.41935778  0.21358547  1.37589109]\n",
      " [ 0.28292236  0.07916144  0.0891926 ]\n",
      " [ 1.04327345  0.76622105  0.06017391]]\n",
      "164 0.611574 [[-2.42318892  0.21212441  1.38118327]\n",
      " [ 0.28301707  0.07958987  0.08866948]\n",
      " [ 1.0447371   0.76657587  0.05835539]]\n",
      "165 0.610619 [[-2.4270134   0.21068141  1.38645065]\n",
      " [ 0.28311282  0.08001768  0.0881459 ]\n",
      " [ 1.0461961   0.76692384  0.05654839]]\n",
      "166 0.609672 [[-2.43083096  0.20925617  1.39169359]\n",
      " [ 0.28320995  0.08044443  0.08762203]\n",
      " [ 1.04765093  0.76726472  0.05475272]]\n",
      "167 0.608733 [[-2.43464208  0.20784856  1.39691222]\n",
      " [ 0.28330803  0.08087041  0.08709797]\n",
      " [ 1.04910123  0.76759911  0.05296808]]\n",
      "168 0.607801 [[-2.43844652  0.2064583   1.40210688]\n",
      " [ 0.28340736  0.08129518  0.08657386]\n",
      " [ 1.05054736  0.76792675  0.05119429]]\n",
      "169 0.606877 [[-2.44224429  0.20508523  1.4072777 ]\n",
      " [ 0.28350762  0.08171899  0.0860498 ]\n",
      " [ 1.0519892   0.76824814  0.04943107]]\n",
      "170 0.60596 [[-2.44603562  0.20372912  1.41242504]\n",
      " [ 0.28360897  0.08214152  0.08552594]\n",
      " [ 1.05342698  0.76856321  0.04767824]]\n",
      "171 0.605051 [[-2.44982028  0.20238981  1.41754913]\n",
      " [ 0.28371119  0.08256288  0.08500236]\n",
      " [ 1.05486059  0.76887226  0.04593556]]\n",
      "172 0.604149 [[-2.4535985   0.20106705  1.42265022]\n",
      " [ 0.28381437  0.08298289  0.08447917]\n",
      " [ 1.05629027  0.76917529  0.04420282]]\n",
      "173 0.603254 [[-2.45737052  0.19976069  1.42772853]\n",
      " [ 0.28391832  0.08340163  0.08395647]\n",
      " [ 1.05771601  0.7694726   0.04247982]]\n",
      "174 0.602365 [[-2.4611361   0.19847052  1.4327842 ]\n",
      " [ 0.28402317  0.08381888  0.08343435]\n",
      " [ 1.05913794  0.76976418  0.04076637]]\n",
      "175 0.601484 [[-2.46489525  0.19719635  1.43781757]\n",
      " [ 0.28412879  0.08423471  0.08291293]\n",
      " [ 1.06055593  0.77005023  0.03906228]]\n",
      "176 0.600609 [[-2.4686482   0.19593801  1.44282877]\n",
      " [ 0.28423512  0.08464902  0.08239228]\n",
      " [ 1.06197023  0.77033085  0.03736735]]\n",
      "177 0.599741 [[-2.4723947   0.19469529  1.44781804]\n",
      " [ 0.28434214  0.08506183  0.08187247]\n",
      " [ 1.06338084  0.77060622  0.03568139]]\n",
      "178 0.59888 [[-2.47613502  0.19346803  1.45278573]\n",
      " [ 0.28444982  0.08547301  0.08135362]\n",
      " [ 1.06478775  0.77087641  0.03400426]]\n",
      "179 0.598025 [[-2.47986913  0.19225606  1.45773184]\n",
      " [ 0.28455809  0.0858826   0.08083577]\n",
      " [ 1.06619108  0.77114159  0.03233574]]\n",
      "180 0.597176 [[-2.48359728  0.19105919  1.46265674]\n",
      " [ 0.28466696  0.08629048  0.08031904]\n",
      " [ 1.06759095  0.77140176  0.03067571]]\n",
      "181 0.596334 [[-2.48731923  0.18987724  1.46756065]\n",
      " [ 0.28477633  0.08669665  0.07980347]\n",
      " [ 1.06898737  0.77165711  0.02902398]]\n",
      "182 0.595497 [[-2.49103498  0.18871006  1.4724437 ]\n",
      " [ 0.28488621  0.08710115  0.0792891 ]\n",
      " [ 1.07038033  0.77190775  0.02738037]]\n",
      "183 0.594667 [[-2.49474478  0.18755747  1.47730613]\n",
      " [ 0.28499657  0.08750383  0.07877605]\n",
      " [ 1.07176995  0.77215374  0.02574477]]\n",
      "184 0.593843 [[-2.49844861  0.18641932  1.48214805]\n",
      " [ 0.28510734  0.08790477  0.07826435]\n",
      " [ 1.07315624  0.77239525  0.02411698]]\n",
      "185 0.593025 [[-2.50214648  0.18529542  1.48696971]\n",
      " [ 0.28521857  0.08830383  0.07775408]\n",
      " [ 1.0745393   0.77263224  0.02249691]]\n",
      "186 0.592212 [[-2.50583839  0.18418562  1.49177134]\n",
      " [ 0.28533009  0.0887011   0.07724529]\n",
      " [ 1.07591915  0.77286494  0.02088439]]\n",
      "187 0.591405 [[-2.50952435  0.18308978  1.49655318]\n",
      " [ 0.28544191  0.08909654  0.07673804]\n",
      " [ 1.07729578  0.7730934   0.01927929]]\n",
      "188 0.590604 [[-2.51320434  0.1820077   1.50131536]\n",
      " [ 0.28555411  0.08948998  0.07623236]\n",
      " [ 1.07866943  0.77331758  0.01768145]]\n",
      "189 0.589809 [[-2.5168786   0.18093929  1.50605798]\n",
      " [ 0.28566647  0.08988169  0.0757283 ]\n",
      " [ 1.08003986  0.77353781  0.01609075]]\n",
      "190 0.589018 [[-2.52054691  0.17988433  1.51078129]\n",
      " [ 0.28577918  0.09027135  0.07522594]\n",
      " [ 1.08140743  0.77375388  0.01450707]]\n",
      "191 0.588234 [[-2.5242095   0.17884272  1.51548553]\n",
      " [ 0.28589195  0.09065921  0.0747253 ]\n",
      " [ 1.0827719   0.77396613  0.01293029]]\n",
      "192 0.587454 [[-2.52786636  0.17781429  1.52017081]\n",
      " [ 0.28600496  0.09104506  0.07422642]\n",
      " [ 1.08413363  0.77417445  0.01136028]]\n",
      "193 0.58668 [[-2.53151751  0.17679891  1.52483737]\n",
      " [ 0.28611809  0.09142901  0.07372934]\n",
      " [ 1.08549237  0.77437907  0.00979692]]\n",
      "194 0.585912 [[-2.53516293  0.1757964   1.52948534]\n",
      " [ 0.28623143  0.09181091  0.07323413]\n",
      " [ 1.08684838  0.77457988  0.00824011]]\n",
      "195 0.585148 [[-2.53880286  0.17480667  1.53411496]\n",
      " [ 0.28634471  0.09219097  0.0727408 ]\n",
      " [ 1.08820152  0.77477717  0.00668973]]\n",
      "196 0.584389 [[-2.54243708  0.17382953  1.53872633]\n",
      " [ 0.28645813  0.09256894  0.07224939]\n",
      " [ 1.08955193  0.77497077  0.00514567]]\n",
      "197 0.583635 [[-2.54606557  0.17286487  1.54331958]\n",
      " [ 0.28657153  0.09294502  0.07175991]\n",
      " [ 1.09089959  0.77516097  0.00360779]]\n",
      "198 0.582886 [[ -2.54968858e+00   1.71912521e-01   1.54789495e+00]\n",
      " [  2.86685050e-01   9.33189839e-02   7.12724254e-02]\n",
      " [  1.09224474e+00   7.75347590e-01   2.07603909e-03]]\n",
      "199 0.582142 [[ -2.55330610e+00   1.70972422e-01   1.55245256e+00]\n",
      " [  2.86798388e-01   9.36911404e-02   7.07869381e-02]\n",
      " [  1.09358704e+00   7.75531054e-01   5.50284167e-04]]\n",
      "200 0.581403 [[ -2.55691814e+00   1.70044348e-01   1.55699265e+00]\n",
      " [  2.86911905e-01   9.40610766e-02   7.03034997e-02]\n",
      " [  1.09492695e+00   7.75710940e-01  -9.69555927e-04]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5],[1, 7, 5],[1, 2, 5],[1, 6, 6],[1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],[0, 1, 0],[0, 1, 0],[1, 0, 0],[1, 0, 0]]\n",
    "\n",
    "\n",
    "# testing set\n",
    "x_test = [[2, 1, 1], [3, 1, 2],[3, 3, 4]]\n",
    "y_test = [[0, 0, 1],[0, 0, 1],[0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# 예측값이 맞는지 확인하기\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    #testing\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning rate가 클경우 nan이 나오는 현상이 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.12906 [[-1.01381755  0.13082397  2.15642881]\n",
      " [ 2.51936507 -3.3371594   1.06212533]\n",
      " [ 3.22010565 -1.92360735  1.13485515]]\n",
      "1 22.9442 [[-2.1261158   0.69332397  2.7062273 ]\n",
      " [-1.57984471 -0.7121594   2.53633571]\n",
      " [-0.89178133  0.88889265  2.43424273]]\n",
      "2 23.6021 [[-1.7511158   1.2558229   1.76872838]\n",
      " [ 0.85765529  1.91283846 -2.52616239]\n",
      " [ 1.54571867  3.7013917  -2.81575632]]\n",
      "3 15.4803 [[-1.37647879  0.31868601  2.33122826]\n",
      " [ 3.294415   -2.02392101 -1.02616262]\n",
      " [ 3.98284054 -0.04823017 -1.50325644]]\n",
      "4 24.9305 [[-2.50147438  0.88118595  2.89372396]\n",
      " [-0.83057642  0.60107899  0.47382903]\n",
      " [-0.14215517  2.76426983 -0.19076061]]\n",
      "5 9.10625 [[-2.12648606  0.09125018  3.30867124]\n",
      " [ 1.60689974 -3.02364397  1.66107547]\n",
      " [ 2.29533267 -0.82039881  0.95642042]]\n",
      "6 14.2129 [[-2.30359197  0.65375018  2.92327714]\n",
      " [ 0.98853815 -0.39864397 -0.34556305]\n",
      " [ 1.54597592  1.99210119 -1.10672283]]\n",
      "7 2.60826 [[-2.99491024  0.91304708  3.35529852]\n",
      " [-2.08851624  1.44763422  0.88521326]\n",
      " [-1.20411277  3.56851315  0.06695437]]\n",
      "8 16.3589 [[-2.61991024  0.0853222   3.80802345]\n",
      " [ 0.34898376 -2.26623917  2.16158676]\n",
      " [ 1.23338723 -0.06762743  1.26559472]]\n",
      "9 18.2526 [[-2.24491167  0.6478222   2.87052488]\n",
      " [ 2.78648043  0.3587606  -2.90090942]\n",
      " [ 3.6708827   2.74487233 -3.98440099]]\n",
      "10 15.3632 [[-3.36628056  1.20669174  3.43302441]\n",
      " [-1.33111715  2.97635937 -1.40091038]\n",
      " [-0.44996691  5.5532217  -2.67190146]]\n",
      "11 27.9479 [[-2.99128056  0.26919246  3.99552369]\n",
      " [ 1.10638261 -0.9611392   0.09908831]\n",
      " [ 1.98753285  1.80322242 -1.35940218]]\n",
      "12 3.90531 [[-3.80761027  0.81386065  4.26718521]\n",
      " [-2.29205585  1.62443066  0.91195762]\n",
      " [-1.65902305  4.53427553 -0.44389868]]\n",
      "13 20.3422 [[-3.43261027 -0.00581169  4.7118578 ]\n",
      " [ 0.14544415 -2.07637405  2.17526197]\n",
      " [ 0.77847695  0.90314341  0.74973345]]\n",
      "14 17.0153 [[-3.05761027  0.55668831  3.7743578 ]\n",
      " [ 2.58294415  0.54862595 -2.88723803]\n",
      " [ 3.21597695  3.71564341 -4.50026655]]\n",
      "15 10.3675 [[-3.86352611  0.80011785  4.33684397]\n",
      " [-0.81429887  2.44589615 -1.38726532]\n",
      " [ 0.24368668  5.37544727 -3.18778014]]\n",
      "16 23.6443 [[-3.48852611 -0.13736236  4.89932442]\n",
      " [ 1.62320113 -1.49156451  0.1126951 ]\n",
      " [ 2.68118668  1.62546682 -1.87529993]]\n",
      "17 7.81479 [[-4.37143517  0.42507422  5.21979713]\n",
      " [-1.96202898  1.13328433  1.07307625]\n",
      " [-1.14599466  4.43775845 -0.86041045]]\n",
      "18 17.1602 [[-3.99643517 -0.28402102  5.55389261]\n",
      " [ 0.47547102 -2.30364537  2.07250619]\n",
      " [ 1.29150534  0.95992684  0.17992139]]\n",
      "19 15.1385 [[-3.62143564  0.27847898  4.61639309]\n",
      " [ 2.91296983  0.32135463 -2.98999286]\n",
      " [ 3.72900248  3.77242684 -5.07007599]]\n",
      "20 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "21 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "22 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "23 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "24 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "25 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "26 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "27 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "28 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "29 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "30 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "31 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "32 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "33 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "34 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "35 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "36 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "37 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "38 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "39 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "40 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "41 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "42 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "43 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "44 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "45 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "46 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "47 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "48 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "49 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "50 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "51 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "52 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "53 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "54 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "55 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "56 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "57 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "58 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "59 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "60 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "61 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "62 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "63 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "64 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "65 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "66 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "67 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "68 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "69 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "70 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "71 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "72 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "73 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "74 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "75 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "76 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "77 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "78 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "79 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "80 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "81 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "82 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "83 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "84 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "85 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "86 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "87 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "88 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "89 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "90 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "91 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "92 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "93 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "94 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "95 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "96 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "97 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "98 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "99 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "100 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "101 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "102 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "103 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "104 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "105 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "106 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "107 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "108 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "109 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "110 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "111 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "112 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "113 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "114 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "115 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "116 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "117 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "118 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "119 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "120 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "121 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "122 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "123 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "124 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "125 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "126 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "127 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "128 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "129 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "130 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "131 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "132 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "133 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "134 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "135 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "136 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "137 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "138 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "139 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "140 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "141 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "142 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "143 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "144 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "145 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "146 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "147 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "148 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "149 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "150 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "151 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "152 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "153 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "154 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "155 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "156 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "157 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "158 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "159 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "160 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "161 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "162 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "163 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "164 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "165 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "166 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "167 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "168 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "169 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "170 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "171 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "172 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "173 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "174 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "175 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "176 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "177 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "178 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "179 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "180 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "181 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "182 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "183 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "184 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "185 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "186 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "187 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "188 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "189 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "190 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "191 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "192 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "193 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "194 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "195 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "196 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "197 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "198 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "199 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "200 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5],[1, 7, 5],[1, 2, 5],[1, 6, 6],[1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],[0, 1, 0],[0, 1, 0],[1, 0, 0],[1, 0, 0]]\n",
    "\n",
    "\n",
    "# testing set\n",
    "x_test = [[2, 1, 1], [3, 1, 2],[3, 3, 4]]\n",
    "y_test = [[0, 0, 1],[0, 0, 1],[0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=1.5).minimize(cost)\n",
    "\n",
    "# 예측값이 맞는지 확인하기\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    #testing\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning rate가 완전히 작을 경우 cost가 거의 같은 값이 나오고 이동이 거의 없는 현상이 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "1 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "2 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "3 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "4 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "5 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "6 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "7 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "8 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "9 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "10 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "11 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "12 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "13 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "14 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "15 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "16 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "17 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "18 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "19 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "20 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "21 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "22 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "23 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "24 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "25 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "26 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "27 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "28 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "29 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "30 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "31 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "32 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "33 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "34 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "35 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "36 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "37 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "38 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "39 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "40 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "41 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "42 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "43 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "44 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "45 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "46 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "47 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "48 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "49 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "50 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "51 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "52 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "53 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "54 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "55 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "56 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "57 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "58 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "59 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "60 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "61 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "62 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "63 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "64 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "65 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "66 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "67 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "68 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "69 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "70 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "71 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "72 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "73 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "74 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "75 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "76 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "77 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "78 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "79 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "80 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "81 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "82 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "83 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "84 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "85 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "86 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "87 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "88 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "89 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "90 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "91 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "92 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "93 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "94 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "95 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "96 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "97 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "98 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "99 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "100 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "101 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "102 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "103 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "104 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "106 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "107 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "108 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "109 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "110 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "111 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "112 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "113 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "114 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "115 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "116 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "117 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "118 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "119 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "120 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "121 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "122 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "123 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "124 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "125 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "126 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "127 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "128 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "129 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "130 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "131 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "132 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "133 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "134 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "135 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "136 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "137 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "138 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "139 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "140 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "141 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "142 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "143 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "144 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "145 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "146 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "147 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "148 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "149 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "150 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "151 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "152 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "153 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "154 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "155 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "156 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "157 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "158 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "159 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "160 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "161 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "162 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "163 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "164 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "165 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "166 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "167 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "168 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "169 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "170 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "171 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "172 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "173 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "174 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "175 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "176 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "177 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "178 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "179 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "180 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "181 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "182 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "183 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "184 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "185 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "186 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "187 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "188 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "189 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "190 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "191 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "192 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "193 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "194 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "195 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "196 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "197 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "198 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "199 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "200 4.51399 [[-0.37378627 -0.24196044  0.60383075]\n",
      " [ 0.33969271 -0.04373804 -0.58526856]\n",
      " [-1.0786556   0.43431997 -1.31637883]]\n",
      "Prediction: [1 1 1]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5],[1, 7, 5],[1, 2, 5],[1, 6, 6],[1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],[0, 1, 0],[0, 1, 0],[1, 0, 0],[1, 0, 0]]\n",
    "\n",
    "\n",
    "# testing set\n",
    "x_test = [[2, 1, 1], [3, 1, 2],[3, 3, 4]]\n",
    "y_test = [[0, 0, 1],[0, 0, 1],[0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# 예측값이 맞는지 확인하기\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    #testing\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-normalized된 input값들을 주게 되면 한쪽 방향으로 치우친 그래프가 나오게 되고 예측을 할 시 어느 순간 팅겨나가는 현상이 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1.87575e+12 \n",
      "Prediction:\n",
      " [[  966736.0625]\n",
      " [ 1945891.25  ]\n",
      " [ 1530813.75  ]\n",
      " [ 1073161.875 ]\n",
      " [ 1264739.125 ]\n",
      " [ 1275378.125 ]\n",
      " [ 1168948.125 ]\n",
      " [ 1488227.375 ]]\n",
      "1 Cost:  2.06085e+27 \n",
      "Prediction:\n",
      " [[ -3.20224142e+13]\n",
      " [ -6.44644015e+13]\n",
      " [ -5.07118197e+13]\n",
      " [ -3.55487187e+13]\n",
      " [ -4.18960638e+13]\n",
      " [ -4.22486915e+13]\n",
      " [ -3.87223891e+13]\n",
      " [ -4.93012963e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[  1.06142609e+21]\n",
      " [  2.13675958e+21]\n",
      " [  1.68091156e+21]\n",
      " [  1.17831013e+21]\n",
      " [  1.38870141e+21]\n",
      " [  1.40038980e+21]\n",
      " [  1.28350563e+21]\n",
      " [  1.63415786e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[ -3.51824000e+28]\n",
      " [ -7.08257786e+28]\n",
      " [ -5.57160854e+28]\n",
      " [ -3.90566815e+28]\n",
      " [ -4.60303842e+28]\n",
      " [ -4.64178119e+28]\n",
      " [ -4.25435305e+28]\n",
      " [ -5.41663747e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[  1.16616812e+36]\n",
      " [  2.34761602e+36]\n",
      " [  1.84678470e+36]\n",
      " [  1.29458635e+36]\n",
      " [  1.52573919e+36]\n",
      " [  1.53858097e+36]\n",
      " [  1.41016273e+36]\n",
      " [  1.79541744e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min-max scale함수를 이용하여 normalized된 input값을 이용할 경우 팅겨나가는 현상이 사라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999999  0.99999999  0.          1.          1.        ]\n",
      " [ 0.70548491  0.70439552  1.          0.71881782  0.83755791]\n",
      " [ 0.54412549  0.50274824  0.57608696  0.606468    0.6606331 ]\n",
      " [ 0.33890353  0.31368023  0.10869565  0.45989134  0.43800918]\n",
      " [ 0.51436     0.42582389  0.30434783  0.58504805  0.42624401]\n",
      " [ 0.49556179  0.42582389  0.31521739  0.48131134  0.49276137]\n",
      " [ 0.11436064  0.          0.20652174  0.22007776  0.18597238]\n",
      " [ 0.          0.07747099  0.5326087   0.          0.        ]]\n",
      "0 Cost:  2.45275 \n",
      "Prediction:\n",
      " [[-1.71137929]\n",
      " [-1.53180051]\n",
      " [-1.02103424]\n",
      " [-0.39743036]\n",
      " [-0.83632261]\n",
      " [-0.70120412]\n",
      " [ 0.07918358]\n",
      " [ 0.3155672 ]]\n",
      "1 Cost:  2.45259 \n",
      "Prediction:\n",
      " [[-1.71130347]\n",
      " [-1.53172946]\n",
      " [-1.02097547]\n",
      " [-0.39738548]\n",
      " [-0.83626878]\n",
      " [-0.70115232]\n",
      " [ 0.07921612]\n",
      " [ 0.31559846]]\n",
      "2 Cost:  2.45244 \n",
      "Prediction:\n",
      " [[-1.71122789]\n",
      " [-1.53165841]\n",
      " [-1.0209167 ]\n",
      " [-0.39734071]\n",
      " [-0.83621496]\n",
      " [-0.70110053]\n",
      " [ 0.07924873]\n",
      " [ 0.31562972]]\n",
      "3 Cost:  2.45228 \n",
      "Prediction:\n",
      " [[-1.71115196]\n",
      " [-1.53158724]\n",
      " [-1.02085793]\n",
      " [-0.39729583]\n",
      " [-0.83616114]\n",
      " [-0.70104873]\n",
      " [ 0.07928133]\n",
      " [ 0.31566101]]\n",
      "4 Cost:  2.45212 \n",
      "Prediction:\n",
      " [[-1.71107602]\n",
      " [-1.53151631]\n",
      " [-1.02079916]\n",
      " [-0.39725095]\n",
      " [-0.83610731]\n",
      " [-0.70099694]\n",
      " [ 0.07931393]\n",
      " [ 0.31569225]]\n",
      "5 Cost:  2.45196 \n",
      "Prediction:\n",
      " [[-1.71100044]\n",
      " [-1.53144503]\n",
      " [-1.02074051]\n",
      " [-0.39720619]\n",
      " [-0.83605337]\n",
      " [-0.70094514]\n",
      " [ 0.07934648]\n",
      " [ 0.31572351]]\n",
      "6 Cost:  2.4518 \n",
      "Prediction:\n",
      " [[-1.71092439]\n",
      " [-1.53137422]\n",
      " [-1.02068186]\n",
      " [-0.39716136]\n",
      " [-0.83599955]\n",
      " [-0.70089346]\n",
      " [ 0.07937908]\n",
      " [ 0.31575477]]\n",
      "7 Cost:  2.45164 \n",
      "Prediction:\n",
      " [[-1.71084869]\n",
      " [-1.53130305]\n",
      " [-1.02062309]\n",
      " [-0.3971166 ]\n",
      " [-0.83594573]\n",
      " [-0.70084167]\n",
      " [ 0.07941169]\n",
      " [ 0.315786  ]]\n",
      "8 Cost:  2.45148 \n",
      "Prediction:\n",
      " [[-1.71077275]\n",
      " [-1.53123188]\n",
      " [-1.02056432]\n",
      " [-0.39707178]\n",
      " [-0.83589178]\n",
      " [-0.70078975]\n",
      " [ 0.07944429]\n",
      " [ 0.31581727]]\n",
      "9 Cost:  2.45133 \n",
      "Prediction:\n",
      " [[-1.71069694]\n",
      " [-1.53116107]\n",
      " [-1.02050555]\n",
      " [-0.3970269 ]\n",
      " [-0.83583808]\n",
      " [-0.70073807]\n",
      " [ 0.07947686]\n",
      " [ 0.31584853]]\n",
      "10 Cost:  2.45117 \n",
      "Prediction:\n",
      " [[-1.71062136]\n",
      " [-1.53109002]\n",
      " [-1.02044678]\n",
      " [-0.39698213]\n",
      " [-0.83578414]\n",
      " [-0.70068628]\n",
      " [ 0.07950944]\n",
      " [ 0.31587976]]\n",
      "11 Cost:  2.45101 \n",
      "Prediction:\n",
      " [[-1.71054542]\n",
      " [-1.53101885]\n",
      " [-1.02038813]\n",
      " [-0.39693725]\n",
      " [-0.83573031]\n",
      " [-0.7006346 ]\n",
      " [ 0.07954204]\n",
      " [ 0.31591105]]\n",
      "12 Cost:  2.45085 \n",
      "Prediction:\n",
      " [[-1.71046948]\n",
      " [-1.53094769]\n",
      " [-1.02032948]\n",
      " [-0.39689249]\n",
      " [-0.83567649]\n",
      " [-0.7005828 ]\n",
      " [ 0.07957461]\n",
      " [ 0.31594232]]\n",
      "13 Cost:  2.45069 \n",
      "Prediction:\n",
      " [[-1.71039391]\n",
      " [-1.53087664]\n",
      " [-1.02027059]\n",
      " [-0.39684767]\n",
      " [-0.83562255]\n",
      " [-0.70053101]\n",
      " [ 0.07960719]\n",
      " [ 0.31597358]]\n",
      "14 Cost:  2.45054 \n",
      "Prediction:\n",
      " [[-1.71031785]\n",
      " [-1.53080583]\n",
      " [-1.02021194]\n",
      " [-0.39680284]\n",
      " [-0.83556885]\n",
      " [-0.70047921]\n",
      " [ 0.07963979]\n",
      " [ 0.31600481]]\n",
      "15 Cost:  2.45038 \n",
      "Prediction:\n",
      " [[-1.71024215]\n",
      " [-1.53073466]\n",
      " [-1.02015316]\n",
      " [-0.39675802]\n",
      " [-0.8355149 ]\n",
      " [-0.70042741]\n",
      " [ 0.0796724 ]\n",
      " [ 0.31603608]]\n",
      "16 Cost:  2.45022 \n",
      "Prediction:\n",
      " [[-1.71016622]\n",
      " [-1.53066349]\n",
      " [-1.02009439]\n",
      " [-0.3967132 ]\n",
      " [-0.83546108]\n",
      " [-0.70037562]\n",
      " [ 0.07970494]\n",
      " [ 0.31606734]]\n",
      "17 Cost:  2.45006 \n",
      "Prediction:\n",
      " [[-1.7100904 ]\n",
      " [-1.53059268]\n",
      " [-1.02003574]\n",
      " [-0.39666837]\n",
      " [-0.83540726]\n",
      " [-0.70032382]\n",
      " [ 0.07973754]\n",
      " [ 0.31609857]]\n",
      "18 Cost:  2.4499 \n",
      "Prediction:\n",
      " [[-1.71001482]\n",
      " [-1.53052139]\n",
      " [-1.01997685]\n",
      " [-0.39662355]\n",
      " [-0.83535331]\n",
      " [-0.70027202]\n",
      " [ 0.07977015]\n",
      " [ 0.31612986]]\n",
      "19 Cost:  2.44974 \n",
      "Prediction:\n",
      " [[-1.70993888]\n",
      " [-1.53045046]\n",
      " [-1.0199182 ]\n",
      " [-0.39657879]\n",
      " [-0.83529961]\n",
      " [-0.70022023]\n",
      " [ 0.07980275]\n",
      " [ 0.3161611 ]]\n",
      "20 Cost:  2.44959 \n",
      "Prediction:\n",
      " [[-1.70986295]\n",
      " [-1.5303793 ]\n",
      " [-1.01985955]\n",
      " [-0.39653403]\n",
      " [-0.83524567]\n",
      " [-0.70016843]\n",
      " [ 0.07983533]\n",
      " [ 0.31619239]]\n",
      "21 Cost:  2.44943 \n",
      "Prediction:\n",
      " [[-1.70978737]\n",
      " [-1.53030825]\n",
      " [-1.01980066]\n",
      " [-0.39648914]\n",
      " [-0.83519197]\n",
      " [-0.70011675]\n",
      " [ 0.0798679 ]\n",
      " [ 0.31622362]]\n",
      "22 Cost:  2.44927 \n",
      "Prediction:\n",
      " [[-1.70971155]\n",
      " [-1.5302372 ]\n",
      " [-1.01974201]\n",
      " [-0.39644438]\n",
      " [-0.83513802]\n",
      " [-0.70006484]\n",
      " [ 0.0799005 ]\n",
      " [ 0.31625488]]\n",
      "23 Cost:  2.44911 \n",
      "Prediction:\n",
      " [[-1.70963562]\n",
      " [-1.53016627]\n",
      " [-1.01968324]\n",
      " [-0.3963995 ]\n",
      " [-0.83508408]\n",
      " [-0.70001304]\n",
      " [ 0.07993308]\n",
      " [ 0.31628615]]\n",
      "24 Cost:  2.44895 \n",
      "Prediction:\n",
      " [[-1.70955992]\n",
      " [-1.5300951 ]\n",
      " [-1.01962447]\n",
      " [-0.39635462]\n",
      " [-0.83503038]\n",
      " [-0.69996136]\n",
      " [ 0.07996565]\n",
      " [ 0.31631738]]\n",
      "25 Cost:  2.44879 \n",
      "Prediction:\n",
      " [[-1.7094841 ]\n",
      " [-1.53002405]\n",
      " [-1.01956582]\n",
      " [-0.39630985]\n",
      " [-0.83497643]\n",
      " [-0.69990957]\n",
      " [ 0.07999825]\n",
      " [ 0.31634867]]\n",
      "26 Cost:  2.44864 \n",
      "Prediction:\n",
      " [[-1.70940828]\n",
      " [-1.529953  ]\n",
      " [-1.01950717]\n",
      " [-0.39626503]\n",
      " [-0.83492273]\n",
      " [-0.69985789]\n",
      " [ 0.08003086]\n",
      " [ 0.3163799 ]]\n",
      "27 Cost:  2.44848 \n",
      "Prediction:\n",
      " [[-1.7093327 ]\n",
      " [-1.52988195]\n",
      " [-1.01944828]\n",
      " [-0.39622033]\n",
      " [-0.83486885]\n",
      " [-0.69980615]\n",
      " [ 0.0800634 ]\n",
      " [ 0.31641114]]\n",
      "28 Cost:  2.44832 \n",
      "Prediction:\n",
      " [[-1.70925665]\n",
      " [-1.52981091]\n",
      " [-1.01938987]\n",
      " [-0.39617556]\n",
      " [-0.83481508]\n",
      " [-0.69975442]\n",
      " [ 0.08009592]\n",
      " [ 0.31644231]]\n",
      "29 Cost:  2.44816 \n",
      "Prediction:\n",
      " [[-1.70918107]\n",
      " [-1.5297401 ]\n",
      " [-1.01933122]\n",
      " [-0.3961308 ]\n",
      " [-0.83476132]\n",
      " [-0.69970268]\n",
      " [ 0.08012843]\n",
      " [ 0.31647351]]\n",
      "30 Cost:  2.448 \n",
      "Prediction:\n",
      " [[-1.70910549]\n",
      " [-1.52966928]\n",
      " [-1.01927233]\n",
      " [-0.39608604]\n",
      " [-0.83470744]\n",
      " [-0.69965094]\n",
      " [ 0.08016098]\n",
      " [ 0.31650472]]\n",
      "31 Cost:  2.44785 \n",
      "Prediction:\n",
      " [[-1.70902944]\n",
      " [-1.52959824]\n",
      " [-1.01921368]\n",
      " [-0.39604121]\n",
      " [-0.83465368]\n",
      " [-0.69959921]\n",
      " [ 0.08019349]\n",
      " [ 0.31653589]]\n",
      "32 Cost:  2.44769 \n",
      "Prediction:\n",
      " [[-1.70895386]\n",
      " [-1.52952719]\n",
      " [-1.01915503]\n",
      " [-0.39599651]\n",
      " [-0.83459991]\n",
      " [-0.69954747]\n",
      " [ 0.08022603]\n",
      " [ 0.31656712]]\n",
      "33 Cost:  2.44753 \n",
      "Prediction:\n",
      " [[-1.70887828]\n",
      " [-1.52945614]\n",
      " [-1.01909637]\n",
      " [-0.39595175]\n",
      " [-0.83454603]\n",
      " [-0.69949573]\n",
      " [ 0.08025855]\n",
      " [ 0.3165983 ]]\n",
      "34 Cost:  2.44737 \n",
      "Prediction:\n",
      " [[-1.70880222]\n",
      " [-1.52938509]\n",
      " [-1.01903772]\n",
      " [-0.39590698]\n",
      " [-0.83449239]\n",
      " [-0.699444  ]\n",
      " [ 0.08029106]\n",
      " [ 0.3166295 ]]\n",
      "35 Cost:  2.44721 \n",
      "Prediction:\n",
      " [[-1.70872664]\n",
      " [-1.52931404]\n",
      " [-1.01897907]\n",
      " [-0.39586222]\n",
      " [-0.8344385 ]\n",
      " [-0.69939238]\n",
      " [ 0.08032361]\n",
      " [ 0.3166607 ]]\n",
      "36 Cost:  2.44706 \n",
      "Prediction:\n",
      " [[-1.70865083]\n",
      " [-1.52924323]\n",
      " [-1.01892042]\n",
      " [-0.39581746]\n",
      " [-0.83438486]\n",
      " [-0.69934064]\n",
      " [ 0.08035612]\n",
      " [ 0.31669188]]\n",
      "37 Cost:  2.4469 \n",
      "Prediction:\n",
      " [[-1.70857501]\n",
      " [-1.52917218]\n",
      " [-1.01886177]\n",
      " [-0.3957727 ]\n",
      " [-0.83433098]\n",
      " [-0.69928879]\n",
      " [ 0.08038867]\n",
      " [ 0.31672308]]\n",
      "38 Cost:  2.44674 \n",
      "Prediction:\n",
      " [[-1.70849943]\n",
      " [-1.52910113]\n",
      " [-1.01880312]\n",
      " [-0.39572793]\n",
      " [-0.83427721]\n",
      " [-0.69923717]\n",
      " [ 0.08042118]\n",
      " [ 0.31675428]]\n",
      "39 Cost:  2.44658 \n",
      "Prediction:\n",
      " [[-1.70842361]\n",
      " [-1.52903032]\n",
      " [-1.01874447]\n",
      " [-0.39568323]\n",
      " [-0.83422345]\n",
      " [-0.69918555]\n",
      " [ 0.08045375]\n",
      " [ 0.31678551]]\n",
      "40 Cost:  2.44642 \n",
      "Prediction:\n",
      " [[-1.7083478 ]\n",
      " [-1.52895927]\n",
      " [-1.01868558]\n",
      " [-0.39563853]\n",
      " [-0.83416957]\n",
      " [-0.69913381]\n",
      " [ 0.08048624]\n",
      " [ 0.31681669]]\n",
      "41 Cost:  2.44627 \n",
      "Prediction:\n",
      " [[-1.70827222]\n",
      " [-1.52888823]\n",
      " [-1.01862717]\n",
      " [-0.3955937 ]\n",
      " [-0.83411592]\n",
      " [-0.69908208]\n",
      " [ 0.08051878]\n",
      " [ 0.31684789]]\n",
      "42 Cost:  2.44611 \n",
      "Prediction:\n",
      " [[-1.7081964 ]\n",
      " [-1.52881718]\n",
      " [-1.01856828]\n",
      " [-0.395549  ]\n",
      " [-0.83406204]\n",
      " [-0.69903034]\n",
      " [ 0.08055133]\n",
      " [ 0.31687906]]\n",
      "43 Cost:  2.44595 \n",
      "Prediction:\n",
      " [[-1.70812058]\n",
      " [-1.52874637]\n",
      " [-1.01850963]\n",
      " [-0.39550418]\n",
      " [-0.8340084 ]\n",
      " [-0.6989786 ]\n",
      " [ 0.08058381]\n",
      " [ 0.31691027]]\n",
      "44 Cost:  2.44579 \n",
      "Prediction:\n",
      " [[-1.70804501]\n",
      " [-1.52867508]\n",
      " [-1.01845098]\n",
      " [-0.39545935]\n",
      " [-0.83395451]\n",
      " [-0.69892687]\n",
      " [ 0.08061635]\n",
      " [ 0.31694144]]\n",
      "45 Cost:  2.44563 \n",
      "Prediction:\n",
      " [[-1.70796919]\n",
      " [-1.52860427]\n",
      " [-1.01839232]\n",
      " [-0.39541465]\n",
      " [-0.83390075]\n",
      " [-0.69887513]\n",
      " [ 0.0806489 ]\n",
      " [ 0.31697267]]\n",
      "46 Cost:  2.44548 \n",
      "Prediction:\n",
      " [[-1.70789337]\n",
      " [-1.52853322]\n",
      " [-1.01833367]\n",
      " [-0.39536995]\n",
      " [-0.83384699]\n",
      " [-0.69882339]\n",
      " [ 0.08068138]\n",
      " [ 0.31700385]]\n",
      "47 Cost:  2.44532 \n",
      "Prediction:\n",
      " [[-1.70781779]\n",
      " [-1.52846217]\n",
      " [-1.01827502]\n",
      " [-0.39532518]\n",
      " [-0.8337931 ]\n",
      " [-0.69877177]\n",
      " [ 0.08071393]\n",
      " [ 0.31703508]]\n",
      "48 Cost:  2.44516 \n",
      "Prediction:\n",
      " [[-1.70774221]\n",
      " [-1.52839136]\n",
      " [-1.01821637]\n",
      " [-0.39528042]\n",
      " [-0.83373946]\n",
      " [-0.69871992]\n",
      " [ 0.08074647]\n",
      " [ 0.31706625]]\n",
      "49 Cost:  2.445 \n",
      "Prediction:\n",
      " [[-1.70766616]\n",
      " [-1.52832031]\n",
      " [-1.01815772]\n",
      " [-0.39523572]\n",
      " [-0.83368558]\n",
      " [-0.69866818]\n",
      " [ 0.08077902]\n",
      " [ 0.31709746]]\n",
      "50 Cost:  2.44484 \n",
      "Prediction:\n",
      " [[-1.70759058]\n",
      " [-1.52824926]\n",
      " [-1.01809907]\n",
      " [-0.39519089]\n",
      " [-0.83363193]\n",
      " [-0.69861656]\n",
      " [ 0.08081156]\n",
      " [ 0.31712866]]\n",
      "51 Cost:  2.44469 \n",
      "Prediction:\n",
      " [[-1.707515  ]\n",
      " [-1.52817845]\n",
      " [-1.01804042]\n",
      " [-0.39514619]\n",
      " [-0.83357805]\n",
      " [-0.69856495]\n",
      " [ 0.08084404]\n",
      " [ 0.31715983]]\n",
      "52 Cost:  2.44453 \n",
      "Prediction:\n",
      " [[-1.70743895]\n",
      " [-1.5281074 ]\n",
      " [-1.01798177]\n",
      " [-0.39510143]\n",
      " [-0.83352429]\n",
      " [-0.69851321]\n",
      " [ 0.08087659]\n",
      " [ 0.317191  ]]\n",
      "53 Cost:  2.44437 \n",
      "Prediction:\n",
      " [[-1.70736337]\n",
      " [-1.52803636]\n",
      " [-1.01792312]\n",
      " [-0.39505672]\n",
      " [-0.83347052]\n",
      " [-0.69846147]\n",
      " [ 0.0809091 ]\n",
      " [ 0.31722218]]\n",
      "54 Cost:  2.44421 \n",
      "Prediction:\n",
      " [[-1.70728779]\n",
      " [-1.52796555]\n",
      " [-1.01786447]\n",
      " [-0.39501196]\n",
      " [-0.83341664]\n",
      " [-0.69840974]\n",
      " [ 0.08094162]\n",
      " [ 0.31725338]]\n",
      "55 Cost:  2.44405 \n",
      "Prediction:\n",
      " [[-1.70721197]\n",
      " [-1.5278945 ]\n",
      " [-1.01780581]\n",
      " [-0.3949672 ]\n",
      " [-0.833363  ]\n",
      " [-0.698358  ]\n",
      " [ 0.08097416]\n",
      " [ 0.31728455]]\n",
      "56 Cost:  2.4439 \n",
      "Prediction:\n",
      " [[-1.70713615]\n",
      " [-1.52782369]\n",
      " [-1.01774716]\n",
      " [-0.39492244]\n",
      " [-0.83330923]\n",
      " [-0.69830626]\n",
      " [ 0.08100668]\n",
      " [ 0.31731576]]\n",
      "57 Cost:  2.44374 \n",
      "Prediction:\n",
      " [[-1.70706058]\n",
      " [-1.52775264]\n",
      " [-1.01768851]\n",
      " [-0.39487773]\n",
      " [-0.83325547]\n",
      " [-0.69825476]\n",
      " [ 0.08103919]\n",
      " [ 0.31734693]]\n",
      "58 Cost:  2.44358 \n",
      "Prediction:\n",
      " [[-1.70698476]\n",
      " [-1.52768159]\n",
      " [-1.01762986]\n",
      " [-0.39483291]\n",
      " [-0.83320171]\n",
      " [-0.69820303]\n",
      " [ 0.0810717 ]\n",
      " [ 0.3173781 ]]\n",
      "59 Cost:  2.44342 \n",
      "Prediction:\n",
      " [[-1.70690894]\n",
      " [-1.52761078]\n",
      " [-1.01757121]\n",
      " [-0.39478821]\n",
      " [-0.83314794]\n",
      " [-0.69815129]\n",
      " [ 0.08110422]\n",
      " [ 0.31740928]]\n",
      "60 Cost:  2.44327 \n",
      "Prediction:\n",
      " [[-1.70683336]\n",
      " [-1.52753973]\n",
      " [-1.01751256]\n",
      " [-0.3947435 ]\n",
      " [-0.83309418]\n",
      " [-0.69809955]\n",
      " [ 0.08113676]\n",
      " [ 0.31744045]]\n",
      "61 Cost:  2.44311 \n",
      "Prediction:\n",
      " [[-1.70675755]\n",
      " [-1.52746892]\n",
      " [-1.01745391]\n",
      " [-0.39469868]\n",
      " [-0.83304042]\n",
      " [-0.69804782]\n",
      " [ 0.08116931]\n",
      " [ 0.31747162]]\n",
      "62 Cost:  2.44295 \n",
      "Prediction:\n",
      " [[-1.70668173]\n",
      " [-1.52739787]\n",
      " [-1.01739526]\n",
      " [-0.39465398]\n",
      " [-0.83298653]\n",
      " [-0.6979962 ]\n",
      " [ 0.08120179]\n",
      " [ 0.3175028 ]]\n",
      "63 Cost:  2.44279 \n",
      "Prediction:\n",
      " [[-1.70660615]\n",
      " [-1.52732682]\n",
      " [-1.01733661]\n",
      " [-0.39460921]\n",
      " [-0.83293289]\n",
      " [-0.69794434]\n",
      " [ 0.08123434]\n",
      " [ 0.317534  ]]\n",
      "64 Cost:  2.44263 \n",
      "Prediction:\n",
      " [[-1.70653033]\n",
      " [-1.52725601]\n",
      " [-1.01727796]\n",
      " [-0.39456445]\n",
      " [-0.83287901]\n",
      " [-0.69789284]\n",
      " [ 0.08126682]\n",
      " [ 0.31756517]]\n",
      "65 Cost:  2.44248 \n",
      "Prediction:\n",
      " [[-1.70645452]\n",
      " [-1.5271852 ]\n",
      " [-1.01721931]\n",
      " [-0.39451969]\n",
      " [-0.83282536]\n",
      " [-0.69784111]\n",
      " [ 0.08129936]\n",
      " [ 0.31759638]]\n",
      "66 Cost:  2.44232 \n",
      "Prediction:\n",
      " [[-1.70637894]\n",
      " [-1.52711391]\n",
      " [-1.01716065]\n",
      " [-0.39447492]\n",
      " [-0.83277148]\n",
      " [-0.69778937]\n",
      " [ 0.08133191]\n",
      " [ 0.31762755]]\n",
      "67 Cost:  2.44216 \n",
      "Prediction:\n",
      " [[-1.70630336]\n",
      " [-1.5270431 ]\n",
      " [-1.017102  ]\n",
      " [-0.39443022]\n",
      " [-0.83271784]\n",
      " [-0.69773763]\n",
      " [ 0.08136439]\n",
      " [ 0.31765875]]\n",
      "68 Cost:  2.442 \n",
      "Prediction:\n",
      " [[-1.70622754]\n",
      " [-1.52697229]\n",
      " [-1.01704335]\n",
      " [-0.39438552]\n",
      " [-0.83266407]\n",
      " [-0.6976859 ]\n",
      " [ 0.08139694]\n",
      " [ 0.31768993]]\n",
      "69 Cost:  2.44185 \n",
      "Prediction:\n",
      " [[-1.70615172]\n",
      " [-1.52690125]\n",
      " [-1.0169847 ]\n",
      " [-0.39434069]\n",
      " [-0.83261031]\n",
      " [-0.69763416]\n",
      " [ 0.08142942]\n",
      " [ 0.3177211 ]]\n",
      "70 Cost:  2.44169 \n",
      "Prediction:\n",
      " [[-1.70607615]\n",
      " [-1.5268302 ]\n",
      " [-1.01692605]\n",
      " [-0.39429599]\n",
      " [-0.83255643]\n",
      " [-0.69758254]\n",
      " [ 0.08146197]\n",
      " [ 0.3177523 ]]\n",
      "71 Cost:  2.44153 \n",
      "Prediction:\n",
      " [[-1.70600033]\n",
      " [-1.52675939]\n",
      " [-1.0168674 ]\n",
      " [-0.39425117]\n",
      " [-0.83250266]\n",
      " [-0.69753081]\n",
      " [ 0.08149451]\n",
      " [ 0.31778347]]\n",
      "72 Cost:  2.44137 \n",
      "Prediction:\n",
      " [[-1.70592451]\n",
      " [-1.52668834]\n",
      " [-1.01680875]\n",
      " [-0.39420646]\n",
      " [-0.8324489 ]\n",
      " [-0.69747919]\n",
      " [ 0.08152702]\n",
      " [ 0.31781465]]\n",
      "73 Cost:  2.44121 \n",
      "Prediction:\n",
      " [[-1.70584893]\n",
      " [-1.52661753]\n",
      " [-1.0167501 ]\n",
      " [-0.39416176]\n",
      " [-0.83239514]\n",
      " [-0.69742745]\n",
      " [ 0.08155954]\n",
      " [ 0.31784582]]\n",
      "74 Cost:  2.44106 \n",
      "Prediction:\n",
      " [[-1.70577312]\n",
      " [-1.52654648]\n",
      " [-1.01669145]\n",
      " [-0.39411694]\n",
      " [-0.83234137]\n",
      " [-0.69737571]\n",
      " [ 0.08159208]\n",
      " [ 0.31787699]]\n",
      "75 Cost:  2.4409 \n",
      "Prediction:\n",
      " [[-1.7056973 ]\n",
      " [-1.52647567]\n",
      " [-1.0166328 ]\n",
      " [-0.39407223]\n",
      " [-0.83228773]\n",
      " [-0.69732398]\n",
      " [ 0.0816246 ]\n",
      " [ 0.31790817]]\n",
      "76 Cost:  2.44074 \n",
      "Prediction:\n",
      " [[-1.70562172]\n",
      " [-1.52640462]\n",
      " [-1.01657414]\n",
      " [-0.39402747]\n",
      " [-0.83223385]\n",
      " [-0.69727236]\n",
      " [ 0.08165711]\n",
      " [ 0.31793934]]\n",
      "77 Cost:  2.44058 \n",
      "Prediction:\n",
      " [[-1.70554614]\n",
      " [-1.52633357]\n",
      " [-1.01651549]\n",
      " [-0.39398271]\n",
      " [-0.83218008]\n",
      " [-0.69722062]\n",
      " [ 0.08168963]\n",
      " [ 0.31797054]]\n",
      "78 Cost:  2.44043 \n",
      "Prediction:\n",
      " [[-1.70547032]\n",
      " [-1.52626276]\n",
      " [-1.01645708]\n",
      " [-0.393938  ]\n",
      " [-0.83212632]\n",
      " [-0.69716889]\n",
      " [ 0.08172214]\n",
      " [ 0.31800172]]\n",
      "79 Cost:  2.44027 \n",
      "Prediction:\n",
      " [[-1.70539451]\n",
      " [-1.52619171]\n",
      " [-1.01639843]\n",
      " [-0.3938933 ]\n",
      " [-0.83207256]\n",
      " [-0.69711727]\n",
      " [ 0.08175468]\n",
      " [ 0.31803289]]\n",
      "80 Cost:  2.44011 \n",
      "Prediction:\n",
      " [[-1.70531893]\n",
      " [-1.5261209 ]\n",
      " [-1.01633978]\n",
      " [-0.3938486 ]\n",
      " [-0.83201879]\n",
      " [-0.69706553]\n",
      " [ 0.0817872 ]\n",
      " [ 0.31806409]]\n",
      "81 Cost:  2.43995 \n",
      "Prediction:\n",
      " [[-1.70524311]\n",
      " [-1.52604985]\n",
      " [-1.01628113]\n",
      " [-0.39380378]\n",
      " [-0.83196503]\n",
      " [-0.6970138 ]\n",
      " [ 0.08181971]\n",
      " [ 0.31809527]]\n",
      "82 Cost:  2.4398 \n",
      "Prediction:\n",
      " [[-1.70516729]\n",
      " [-1.52597904]\n",
      " [-1.01622248]\n",
      " [-0.39375907]\n",
      " [-0.83191127]\n",
      " [-0.69696206]\n",
      " [ 0.08185226]\n",
      " [ 0.31812644]]\n",
      "83 Cost:  2.43964 \n",
      "Prediction:\n",
      " [[-1.70509171]\n",
      " [-1.52590799]\n",
      " [-1.01616383]\n",
      " [-0.39371431]\n",
      " [-0.8318575 ]\n",
      " [-0.69691044]\n",
      " [ 0.0818848 ]\n",
      " [ 0.31815761]]\n",
      "84 Cost:  2.43948 \n",
      "Prediction:\n",
      " [[-1.70501614]\n",
      " [-1.52583694]\n",
      " [-1.01610518]\n",
      " [-0.39366961]\n",
      " [-0.8318038 ]\n",
      " [-0.69685888]\n",
      " [ 0.08191723]\n",
      " [ 0.31818873]]\n",
      "85 Cost:  2.43932 \n",
      "Prediction:\n",
      " [[-1.70494032]\n",
      " [-1.52576613]\n",
      " [-1.01604652]\n",
      " [-0.39362496]\n",
      " [-0.83175009]\n",
      " [-0.69680721]\n",
      " [ 0.08194971]\n",
      " [ 0.31821984]]\n",
      "86 Cost:  2.43916 \n",
      "Prediction:\n",
      " [[-1.70486462]\n",
      " [-1.52569544]\n",
      " [-1.01598799]\n",
      " [-0.39358026]\n",
      " [-0.83169639]\n",
      " [-0.69675553]\n",
      " [ 0.08198214]\n",
      " [ 0.31825095]]\n",
      "87 Cost:  2.43901 \n",
      "Prediction:\n",
      " [[-1.70478916]\n",
      " [-1.52562451]\n",
      " [-1.01592946]\n",
      " [-0.39353561]\n",
      " [-0.83164269]\n",
      " [-0.69670385]\n",
      " [ 0.08201462]\n",
      " [ 0.31828207]]\n",
      "88 Cost:  2.43885 \n",
      "Prediction:\n",
      " [[-1.70471334]\n",
      " [-1.52555346]\n",
      " [-1.01587081]\n",
      " [-0.39349091]\n",
      " [-0.83158898]\n",
      " [-0.69665217]\n",
      " [ 0.0820471 ]\n",
      " [ 0.31831321]]\n",
      "89 Cost:  2.43869 \n",
      "Prediction:\n",
      " [[-1.70463753]\n",
      " [-1.52548265]\n",
      " [-1.0158124 ]\n",
      " [-0.39344621]\n",
      " [-0.83153516]\n",
      " [-0.6966005 ]\n",
      " [ 0.08207953]\n",
      " [ 0.31834432]]\n",
      "90 Cost:  2.43854 \n",
      "Prediction:\n",
      " [[-1.70456207]\n",
      " [-1.52541196]\n",
      " [-1.01575363]\n",
      " [-0.39340156]\n",
      " [-0.83148158]\n",
      " [-0.69654906]\n",
      " [ 0.08211201]\n",
      " [ 0.31837544]]\n",
      "91 Cost:  2.43838 \n",
      "Prediction:\n",
      " [[-1.70448661]\n",
      " [-1.52534103]\n",
      " [-1.0156951 ]\n",
      " [-0.39335686]\n",
      " [-0.83142787]\n",
      " [-0.69649738]\n",
      " [ 0.08214447]\n",
      " [ 0.31840658]]\n",
      "92 Cost:  2.43822 \n",
      "Prediction:\n",
      " [[-1.70441079]\n",
      " [-1.52526999]\n",
      " [-1.01563644]\n",
      " [-0.39331216]\n",
      " [-0.83137417]\n",
      " [-0.6964457 ]\n",
      " [ 0.08217692]\n",
      " [ 0.3184377 ]]\n",
      "93 Cost:  2.43806 \n",
      "Prediction:\n",
      " [[-1.70433497]\n",
      " [-1.52519917]\n",
      " [-1.01557779]\n",
      " [-0.39326745]\n",
      " [-0.83132058]\n",
      " [-0.69639403]\n",
      " [ 0.08220941]\n",
      " [ 0.31846881]]\n",
      "94 Cost:  2.43791 \n",
      "Prediction:\n",
      " [[-1.70425951]\n",
      " [-1.52512825]\n",
      " [-1.01551938]\n",
      " [-0.39322281]\n",
      " [-0.83126676]\n",
      " [-0.69634247]\n",
      " [ 0.08224186]\n",
      " [ 0.31849992]]\n",
      "95 Cost:  2.43775 \n",
      "Prediction:\n",
      " [[-1.70418406]\n",
      " [-1.52505732]\n",
      " [-1.01546097]\n",
      " [-0.39317816]\n",
      " [-0.83121318]\n",
      " [-0.69629091]\n",
      " [ 0.08227432]\n",
      " [ 0.31853104]]\n",
      "96 Cost:  2.43759 \n",
      "Prediction:\n",
      " [[-1.704108  ]\n",
      " [-1.52498651]\n",
      " [-1.01540232]\n",
      " [-0.3931334 ]\n",
      " [-0.83115935]\n",
      " [-0.69623923]\n",
      " [ 0.0823068 ]\n",
      " [ 0.31856215]]\n",
      "97 Cost:  2.43743 \n",
      "Prediction:\n",
      " [[-1.70403242]\n",
      " [-1.5249157 ]\n",
      " [-1.01534367]\n",
      " [-0.39308876]\n",
      " [-0.83110577]\n",
      " [-0.69618756]\n",
      " [ 0.08233926]\n",
      " [ 0.31859326]]\n",
      "98 Cost:  2.43728 \n",
      "Prediction:\n",
      " [[-1.70395696]\n",
      " [-1.52484477]\n",
      " [-1.01528513]\n",
      " [-0.39304411]\n",
      " [-0.83105195]\n",
      " [-0.69613588]\n",
      " [ 0.08237171]\n",
      " [ 0.31862438]]\n",
      "99 Cost:  2.43712 \n",
      "Prediction:\n",
      " [[-1.70388126]\n",
      " [-1.52477407]\n",
      " [-1.0152266 ]\n",
      " [-0.39299935]\n",
      " [-0.83099824]\n",
      " [-0.6960842 ]\n",
      " [ 0.08240417]\n",
      " [ 0.31865552]]\n",
      "100 Cost:  2.43696 \n",
      "Prediction:\n",
      " [[-1.70380545]\n",
      " [-1.52470303]\n",
      " [-1.01516795]\n",
      " [-0.39295471]\n",
      " [-0.83094454]\n",
      " [-0.69603252]\n",
      " [ 0.08243662]\n",
      " [ 0.31868663]]\n"
     ]
    }
   ],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Meet MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":이미지(글자) 데이터를 가져와서 이 글자를 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 2.725033509\n",
      "Epoch: 0002 cost = 1.048802603\n",
      "Epoch: 0003 cost = 0.837818844\n",
      "Epoch: 0004 cost = 0.735638520\n",
      "Epoch: 0005 cost = 0.671945115\n",
      "Epoch: 0006 cost = 0.626668574\n",
      "Epoch: 0007 cost = 0.592652842\n",
      "Epoch: 0008 cost = 0.564918966\n",
      "Epoch: 0009 cost = 0.542093165\n",
      "Epoch: 0010 cost = 0.522675276\n",
      "Epoch: 0011 cost = 0.506118180\n",
      "Epoch: 0012 cost = 0.491395916\n",
      "Epoch: 0013 cost = 0.479053673\n",
      "Epoch: 0014 cost = 0.467385073\n",
      "Epoch: 0015 cost = 0.456814755\n",
      "Learning finished\n",
      "Accuracy:  0.8897\n",
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncD0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eca1398978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "nb_classes = 10\n",
    "# MNIST data image 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15   #15번 학습시킨다.\n",
    "batch_size = 100  #한번에 조금씩 잘라서 예측한다.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28),cmap='Greys',interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
